{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helvetia1975/JAY_INFO4670_SPRING2026_ASSIGNMENT2/blob/main/Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFKagHf5X0xT"
      },
      "source": [
        "#1. Data Quality Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cqU_aeRX4mi"
      },
      "source": [
        "1.1 Using Python (pandas, matplotlib, or seaborn), load and inspect the Assignment 2 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "lAnE6ozGXM90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "fe4f6474-c30d-49f7-a4e4-aa44a6fa23e4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f8fccd7e-98a4-4b0e-9b36-758715c13162\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f8fccd7e-98a4-4b0e-9b36-758715c13162\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Assignment 2 dataset.csv to Assignment 2 dataset (3).csv\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyyGaVkvMnc5"
      },
      "source": [
        "Write code to explore the data distribution (e.g., region, type, year) and check whether there is any bias. Provide both the code and your interpretation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVVhamUrMnc5",
        "outputId": "6353a77f-4a43-48ba-afaf-00bf1b86a36e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Region Distribution:\n",
            "region\n",
            "WestTexNewMexico       340\n",
            "Albany                 338\n",
            "BaltimoreWashington    338\n",
            "Boise                  338\n",
            "Boston                 338\n",
            "Atlanta                338\n",
            "California             338\n",
            "Charlotte              338\n",
            "Chicago                338\n",
            "CincinnatiDayton       338\n",
            "Columbus               338\n",
            "DallasFtWorth          338\n",
            "Denver                 338\n",
            "Detroit                338\n",
            "GrandRapids            338\n",
            "GreatLakes             338\n",
            "HarrisburgScranton     338\n",
            "HartfordSpringfield    338\n",
            "Houston                338\n",
            "Indianapolis           338\n",
            "Jacksonville           338\n",
            "BuffaloRochester       338\n",
            "LasVegas               338\n",
            "LosAngeles             338\n",
            "MiamiFtLauderdale      338\n",
            "Louisville             338\n",
            "Nashville              338\n",
            "NewOrleansMobile       338\n",
            "NewYork                338\n",
            "Midsouth               338\n",
            "NorthernNewEngland     338\n",
            "Orlando                338\n",
            "Philadelphia           338\n",
            "PhoenixTucson          338\n",
            "Pittsburgh             338\n",
            "Plains                 338\n",
            "Portland               338\n",
            "Northeast              338\n",
            "RaleighGreensboro      338\n",
            "RichmondNorfolk        338\n",
            "Sacramento             338\n",
            "Roanoke                338\n",
            "SanFrancisco           338\n",
            "Seattle                338\n",
            "SouthCarolina          338\n",
            "SanDiego               338\n",
            "SouthCentral           338\n",
            "Southeast              338\n",
            "StLouis                338\n",
            "Spokane                338\n",
            "Syracuse               338\n",
            "Tampa                  338\n",
            "TotalUS                338\n",
            "West                   338\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Region Statistics:\n",
            "Mean: 338.037037037037\n",
            "Standard Deviation: 0.27216552697590907\n",
            "\n",
            "Type Distribution:\n",
            "type\n",
            "organic         9127\n",
            "conventional    9126\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Type Percentage:\n",
            "type\n",
            "organic         50.002739\n",
            "conventional    49.997261\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Year Distribution:\n",
            "year\n",
            "1904       1\n",
            "2015    5615\n",
            "2016    5616\n",
            "2017    5722\n",
            "2018    1300\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('Assignment 2 dataset.csv')\n",
        "\n",
        "region_counts = df['region'].value_counts() # count how many times a region appears\n",
        "print(\"\\nRegion Distribution:\")\n",
        "print(region_counts)\n",
        "\n",
        "print(\"\\nRegion Statistics:\") # calculates mean and standard deviation of region\n",
        "print(\"Mean:\", region_counts.mean())\n",
        "print(\"Standard Deviation:\", region_counts.std())\n",
        "\n",
        "type_counts = df['type'].value_counts() # count how many times a type appears\n",
        "type_percent = df['type'].value_counts(normalize=True) * 100 # type count as a percentage\n",
        "\n",
        "print(\"\\nType Distribution:\")\n",
        "print(type_counts)\n",
        "\n",
        "print(\"\\nType Percentage:\")\n",
        "print(type_percent)\n",
        "\n",
        "year_counts = df['year'].value_counts().sort_index() # counts how many times a year appears\n",
        "print(\"\\nYear Distribution:\")\n",
        "print(year_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My code displays the frequencies for each entry in the region, type, and year columns.\n",
        "For the region column, we see that each region in the dataset appears mostly the same amount, with the exception of WestTexNewMexico at 340 times. It is fair to say that there is a minimal amount of bias in the region column.\n",
        "For the type column, we see that there are only two types, either organic or conventional. Their count is very close to each other, meaning data is distributed almost evenly.\n",
        "For the year column, we see some variation among the listed years. Noticeably, we see 1904 and 2018 have considerably less appearances than the others. This indicates there is bias in this column."
      ],
      "metadata": {
        "id": "yjVLSz3FWu9x"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lAUTHNsX7VK"
      },
      "source": [
        "1.2 Write Python code to check for duplicate rows and missing values in the dataset. Show the number of duplicates and missing values for each column. Then, explain (in comments or markdown) how you would handle these issues (e.g., drop, impute, or replace)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "5g12xgZIYBpS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a20095ac-422d-41e0-deb8-7c9b4452443d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Duplicate Rows:\n",
            "       Column 1       Date  AveragePrice  Total Volume     4046     4225  \\\n",
            "18244         7   2-4-2018          1.63      17074.83  2046.96  1529.20   \n",
            "18247        10  1-14-2018          1.93      16205.22  1527.63  2981.04   \n",
            "18249        10  1-14-2018          1.93      16205.22  1527.63  2981.04   \n",
            "18253         7   2-4-2018          1.63      17074.83  2046.96  1529.20   \n",
            "\n",
            "         4770  Total Bags  Small Bags  Large Bags  XLarge Bags     type  year  \\\n",
            "18244    0.00    13498.67    13066.82      431.85          0.0  organic  2018   \n",
            "18247  727.01    10969.54    10919.54       50.00          0.0  organic  2018   \n",
            "18249  727.01    10969.54    10919.54       50.00          0.0  organic  2018   \n",
            "18253    0.00    13498.67    13066.82      431.85          0.0  organic  2018   \n",
            "\n",
            "                 region  \n",
            "18244  WestTexNewMexico  \n",
            "18247  WestTexNewMexico  \n",
            "18249  WestTexNewMexico  \n",
            "18253  WestTexNewMexico  \n",
            "\n",
            "Number of duplicates: 2\n",
            "\n",
            "Missing Values by Column:\n",
            "Column 1        0\n",
            "Date            0\n",
            "AveragePrice    0\n",
            "Total Volume    1\n",
            "4046            2\n",
            "4225            1\n",
            "4770            1\n",
            "Total Bags      1\n",
            "Small Bags      2\n",
            "Large Bags      2\n",
            "XLarge Bags     1\n",
            "type            1\n",
            "year            0\n",
            "region          0\n",
            "dtype: int64\n",
            "\n",
            "Total missing values: 12\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('Assignment 2 dataset.csv')\n",
        "\n",
        "print(\"\\nDuplicate Rows:\")\n",
        "num_duplicates = df.duplicated().sum() # check for duplicate rows\n",
        "if num_duplicates > 0: # display the duplicatae rows\n",
        "    print(df[df.duplicated(keep=False)]) # checks for duplicate row (if duplicate, mark as True) and only displays rows that return as True\n",
        "print(f\"\\nNumber of duplicates: {num_duplicates}\")\n",
        "\n",
        "print(\"\\nMissing Values by Column:\") # check for missing entries\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "\n",
        "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the duplicate rows, I would simply drop any duplicate rows. As for the missing values, you could drop the row if it has missing data, but you would lose data. Instead of dropping the data, you could instead insert the mean of the data or the mode of the data."
      ],
      "metadata": {
        "id": "BGJogmB6hZat"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HR99RQCYCAK"
      },
      "source": [
        "1.3 Use Python code to print the number of rows and columns in the dataset (e.g., with df.shape). Based on the dataset size, explain (briefly) whether you think the dataset is sufficient for training a machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "id": "qCX1CYR-YEfi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b337cde9-333e-4af3-861b-940630c6711c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 18254\n",
            "Number of columns: 14\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('Assignment 2 dataset.csv')\n",
        "\n",
        "print(f\"Number of rows: {df.shape[0]}\")\n",
        "print(f\"Number of columns: {df.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is suitable for a machine learning model, since it has over 1000 samples and has more than 10 features (columns). This lets the model learn with sufficient amount of information. There are also minimal amounts missing data, bias, and duplicates so data quality is good."
      ],
      "metadata": {
        "id": "WSCnx89G4dCS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XPy7b93Zz7y"
      },
      "source": [
        "#2. Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcDyYNbGZ3s5"
      },
      "source": [
        "2.1 Remove the first column or “Column 1” from the dataset. Treat the ‘year’ variable as nominal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "CNhoAKHfZ6iJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec31c0d1-e1b5-45f4-8390-cd16a33ec3e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before cleaning:\n",
            "Shape: (18254, 14)\n",
            "Columns: ['Column 1', 'Date', 'AveragePrice', 'Total Volume', '4046', '4225', '4770', 'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type', 'year', 'region']\n",
            "\n",
            "After cleaning:\n",
            "Shape: (18254, 13)\n",
            "Year dtype: object\n",
            "Columns: ['Date', 'AveragePrice', 'Total Volume', '4046', '4225', '4770', 'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags', 'type', 'year', 'region']\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('Assignment 2 dataset.csv')\n",
        "\n",
        "print(\"Before cleaning:\") # dataset before changes\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "df = df.drop(df.columns[0], axis=1) # remove the first column \"Column 1\"\n",
        "\n",
        "df['year'] = df['year'].astype(str) # change year to nominal\n",
        "\n",
        "print(\"\\nAfter cleaning:\") # dataset after changes\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Year dtype: {df['year'].dtype}\") # check year type\n",
        "print(f\"Columns: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0CnGYUdZ60p"
      },
      "source": [
        "2.2 Check for duplicate values and remove them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "6jZ92vdvZ82J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aae35fc-f073-4b58-cc47-94be8aba6925"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before removing duplicates:\n",
            "Total rows: 18254\n",
            "            Date  AveragePrice  Total Volume     4046     4225    4770  \\\n",
            "18244   2-4-2018          1.63      17074.83  2046.96  1529.20    0.00   \n",
            "18247  1-14-2018          1.93      16205.22  1527.63  2981.04  727.01   \n",
            "18249  1-14-2018          1.93      16205.22  1527.63  2981.04  727.01   \n",
            "18253   2-4-2018          1.63      17074.83  2046.96  1529.20    0.00   \n",
            "\n",
            "       Total Bags  Small Bags  Large Bags  XLarge Bags     type  year  \\\n",
            "18244    13498.67    13066.82      431.85          0.0  organic  2018   \n",
            "18247    10969.54    10919.54       50.00          0.0  organic  2018   \n",
            "18249    10969.54    10919.54       50.00          0.0  organic  2018   \n",
            "18253    13498.67    13066.82      431.85          0.0  organic  2018   \n",
            "\n",
            "                 region  \n",
            "18244  WestTexNewMexico  \n",
            "18247  WestTexNewMexico  \n",
            "18249  WestTexNewMexico  \n",
            "18253  WestTexNewMexico  \n",
            "Duplicate rows: 2\n",
            "\n",
            "After removing duplicates:\n",
            "Total rows: 18252\n",
            "Rows removed: 2\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Before removing duplicates:\") # dataset before potential drops\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "duplicates = df.duplicated().sum() # check for duplicates\n",
        "if num_duplicates > 0: # display the duplicatae rows\n",
        "    print(df[df.duplicated(keep=False)]) # checks for duplicate row (if duplicate, mark as True) and only displays rows that return as True\n",
        "print(f\"Duplicate rows: {duplicates}\") # list dupliactes\n",
        "\n",
        "df = df.drop_duplicates() # drop duplicates\n",
        "\n",
        "print(\"\\nAfter removing duplicates:\") # check if duplicated rows were deleted\n",
        "print(f\"Total rows: {len(df)}\")\n",
        "print(f\"Rows removed: {duplicates}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIVjpXq3Z9Mh"
      },
      "source": [
        "2.3 Check for missing values. If a data record (row) only has a few missing values, replace the missing values with the median of the column feature in that specific “Region” variable. If most column values in a data record are missing, remove the data record."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "DjCqQrfKaAEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "265c3da0-5d29-4a12-c7d0-cc5828c1f999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial missing values:\n",
            "Date            0\n",
            "AveragePrice    0\n",
            "Total Volume    1\n",
            "4046            2\n",
            "4225            1\n",
            "4770            1\n",
            "Total Bags      1\n",
            "Small Bags      2\n",
            "Large Bags      2\n",
            "XLarge Bags     1\n",
            "type            1\n",
            "year            0\n",
            "region          0\n",
            "dtype: int64\n",
            "\n",
            "Total missing: 12\n",
            "Initial dataset size: 18252 rows\n",
            "\n",
            "Rows with greater than 6 missing values: 1\n",
            "\n",
            "Final missing values:\n",
            "Date            0\n",
            "AveragePrice    0\n",
            "Total Volume    0\n",
            "4046            0\n",
            "4225            0\n",
            "4770            0\n",
            "Total Bags      0\n",
            "Small Bags      0\n",
            "Large Bags      0\n",
            "XLarge Bags     0\n",
            "type            0\n",
            "year            0\n",
            "region          0\n",
            "dtype: int64\n",
            "\n",
            "Total missing: 0\n",
            "Final dataset size: 18251 rows\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Initial missing values:\") # initial dataset state\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nTotal missing: {df.isnull().sum().sum()}\")\n",
        "print(f\"Initial dataset size: {len(df)} rows\")\n",
        "\n",
        "missing_per_row = df.isnull().sum(axis=1) # remove rows with more than 50% of values missing\n",
        "threshold = len(df.columns) * 0.5  # 50% threshold\n",
        "rows_to_remove = missing_per_row > threshold\n",
        "\n",
        "print(f\"\\nRows with greater than {threshold:.0f} missing values: {rows_to_remove.sum()}\")\n",
        "df = df[~rows_to_remove] # the NOT operator (~) flips Boolean values (False -> True and True -> False); this makes the rows_to_remove rows False and df[] keeps only True rows\n",
        "\n",
        "numeric_cols = df.select_dtypes(include=[np.number]).columns # select only numeric columns\n",
        "\n",
        "for col in numeric_cols: # replace missing values with a region-specific median\n",
        "    if df[col].isnull().any():\n",
        "        for region in df['region'].unique(): # loop through the region column and list each unique region\n",
        "            region_median = df[df['region'] == region][col].median() # calculate the median for that region\n",
        "\n",
        "            mask = (df['region'] == region) & (df[col].isnull()) # checks if the row is in the same region and is missing a value\n",
        "            df.loc[mask, col] = region_median # selects specific cells with missing values and have the same region, and fills that cell with the median for that region\n",
        "\n",
        "print(\"\\nFinal missing values:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nTotal missing: {df.isnull().sum().sum()}\")\n",
        "print(f\"Final dataset size: {len(df)} rows\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfu21OWmaAf7"
      },
      "source": [
        "2.4 Find the correlation between the variables and describe how the correlated values among the variables impact the model accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "QLsEOBvcaDxh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb1d1c4-26a0-41e8-9f5f-5620e51c5fb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation Matrix:\n",
            "              AveragePrice  Total Volume   4046   4225   4770  Total Bags  \\\n",
            "AveragePrice         1.000        -0.193 -0.208 -0.173 -0.179      -0.177   \n",
            "Total Volume        -0.193         1.000  0.978  0.974  0.872       0.963   \n",
            "4046                -0.208         0.978  1.000  0.926  0.833       0.920   \n",
            "4225                -0.173         0.974  0.926  1.000  0.888       0.906   \n",
            "4770                -0.179         0.872  0.833  0.888  1.000       0.792   \n",
            "Total Bags          -0.177         0.963  0.920  0.906  0.792       1.000   \n",
            "Small Bags          -0.175         0.967  0.925  0.916  0.803       0.994   \n",
            "Large Bags          -0.173         0.881  0.839  0.810  0.698       0.943   \n",
            "XLarge Bags         -0.118         0.747  0.699  0.689  0.680       0.804   \n",
            "\n",
            "              Small Bags  Large Bags  XLarge Bags  \n",
            "AveragePrice      -0.175      -0.173       -0.118  \n",
            "Total Volume       0.967       0.881        0.747  \n",
            "4046               0.925       0.839        0.699  \n",
            "4225               0.916       0.810        0.689  \n",
            "4770               0.803       0.698        0.680  \n",
            "Total Bags         0.994       0.943        0.804  \n",
            "Small Bags         1.000       0.903        0.807  \n",
            "Large Bags         0.903       1.000        0.711  \n",
            "XLarge Bags        0.807       0.711        1.000  \n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "numeric_df = df.select_dtypes(include=[np.number]) # calculate correlation matrix for numeric columns\n",
        "correlation_matrix = numeric_df.corr()\n",
        "\n",
        "print(\"Correlation Matrix:\") # display correlation matrix\n",
        "print(correlation_matrix.round(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation measures the linear relationship between two variables. If two variables have high correlation, it makes it hard to tell the influence one has on the other. Models may misattribute the effects of one variable onto another."
      ],
      "metadata": {
        "id": "gMB3pioX2ILw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCUOcJyzaEbJ"
      },
      "source": [
        "#3. Exploratory Data Analysis (EDA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cn0ad9wIaJ5p"
      },
      "source": [
        "3.1 Describe the variables\n",
        "- Describe all variables in the dataset.\n",
        "- For continuous variables: report **range (min, max), mean, median, and distribution**.\n",
        "- For categorical variables: list unique values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "q-L_L_FweWXu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72bcfad8-9e69-4c1d-a24c-645470d21950"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Numeric Variables:\n",
            "\n",
            "AveragePrice:\n",
            "  Range:       [0.44, 3.25]\n",
            "  Mean:        1.41\n",
            "  Median:      1.37\n",
            "  Distribution: right-skewed (most values on left, long tail on right)\n",
            "\n",
            "Total Volume:\n",
            "  Range:       [84.56, 62505646.52]\n",
            "  Mean:        850552.31\n",
            "  Median:      107354.25\n",
            "  Distribution: right-skewed (most values on left, long tail on right)\n",
            "\n",
            "4046:\n",
            "  Range:       [0.00, 22743616.17]\n",
            "  Mean:        292983.95\n",
            "  Median:      8645.30\n",
            "  Distribution: right-skewed (most values on left, long tail on right)\n",
            "\n",
            "4225:\n",
            "  Range:       [0.00, 20470572.61]\n",
            "  Mean:        295122.55\n",
            "  Median:      29056.73\n",
            "  Distribution: right-skewed (most values on left, long tail on right)\n",
            "\n",
            "4770:\n",
            "  Range:       [0.00, 2546439.11]\n",
            "  Mean:        22837.27\n",
            "  Median:      184.99\n",
            "  Distribution: right-skewed (most values on left, long tail on right)\n",
            "\n",
            "Total Bags:\n",
            "  Range:       [0.00, 19373134.37]\n",
            "  Mean:        239613.96\n",
            "  Median:      39738.53\n",
            "  Distribution: right-skewed (most values on left, long tail on right)\n",
            "\n",
            "Small Bags:\n",
            "  Range:       [0.00, 13384586.80]\n",
            "  Mean:        182178.42\n",
            "  Median:      26362.82\n",
            "  Distribution: right-skewed (most values on left, long tail on right)\n",
            "\n",
            "Large Bags:\n",
            "  Range:       [0.00, 5719096.61]\n",
            "  Mean:        54332.33\n",
            "  Median:      2647.71\n",
            "  Distribution: right-skewed (most values on left, long tail on right)\n",
            "\n",
            "XLarge Bags:\n",
            "  Range:       [0.00, 551693.65]\n",
            "  Mean:        3106.09\n",
            "  Median:      0.00\n",
            "  Distribution: right-skewed (most values on left, long tail on right)\n",
            "\n",
            "Categorical Variables:\n",
            "\n",
            "Date:\n",
            "  Number of unique values: 170\n",
            "  Unique values: ['1-1-2017', '1-10-2016', '1-11-2015', '1-14-2018', '1-15-2017', '1-17-2016', '1-18-2015', '1-21-1904', '1-21-2018', '1-22-2017', '1-24-2016', '1-25-2015', '1-28-2018', '1-29-2017', '1-3-2016', '1-31-2016', '1-4-2015', '1-7-2018', '1-8-2017', '10-1-2017', '10-11-2015', '10-15-2017', '10-16-2016', '10-18-2015', '10-2-2016', '10-22-2017', '10-23-2016', '10-25-2015', '10-29-2017', '10-30-2016', '10-4-2015', '10-8-2017', '10-9-2016', '11-1-2015', '11-12-2017', '11-13-2016', '11-15-2015', '11-19-2017', '11-20-2016', '11-22-2015', '11-26-2017', '11-27-2016', '11-29-2015', '11-5-2017', '11-6-2016', '11-8-2015', '12-10-2017', '12-11-2016', '12-13-2015', '12-17-2017', '12-18-2016', '12-20-2015', '12-24-2017', '12-25-2016', '12-27-2015', '12-3-2017', '12-31-2017', '12-4-2016', '12-6-2015', '2-1-2015', '2-11-2018', '2-12-2017', '2-14-2016', '2-15-2015', '2-18-2018', '2-19-2017', '2-21-2016', '2-22-2015', '2-25-2018', '2-26-2017', '2-28-2016', '2-4-2018', '2-5-2017', '2-7-2016', '2-8-2015', '3-1-2015', '3-11-2018', '3-12-2017', '3-13-2016', '3-15-2015', '3-18-2018', '3-19-2017', '3-20-2016', '3-22-2015', '3-25-2018', '3-26-2017', '3-27-2016', '3-29-2015', '3-4-2018', '3-5-2017', '3-6-2016', '3-8-2015', '4-10-2016', '4-12-2015', '4-16-2017', '4-17-2016', '4-19-2015', '4-2-2017', '4-23-2017', '4-24-2016', '4-26-2015', '4-3-2016', '4-30-2017', '4-5-2015', '4-9-2017', '5-1-2016', '5-10-2015', '5-14-2017', '5-15-2016', '5-17-2015', '5-21-2017', '5-22-2016', '5-24-2015', '5-28-2017', '5-29-2016', '5-3-2015', '5-31-2015', '5-7-2017', '5-8-2016', '6-11-2017', '6-12-2016', '6-14-2015', '6-18-2017', '6-19-2016', '6-21-2015', '6-25-2017', '6-26-2016', '6-28-2015', '6-4-2017', '6-5-2016', '6-7-2015', '7-10-2016', '7-12-2015', '7-16-2017', '7-17-2016', '7-19-2015', '7-2-2017', '7-23-2017', '7-24-2016', '7-26-2015', '7-3-2016', '7-30-2017', '7-31-2016', '7-5-2015', '7-9-2017', '8-13-2017', '8-14-2016', '8-16-2015', '8-2-2015', '8-20-2017', '8-21-2016', '8-23-2015', '8-27-2017', '8-28-2016', '8-30-2015', '8-6-2017', '8-7-2016', '8-9-2015', '9-10-2017', '9-11-2016', '9-13-2015', '9-17-2017', '9-18-2016', '9-20-2015', '9-24-2017', '9-25-2016', '9-27-2015', '9-3-2017', '9-4-2016', '9-6-2015']\n",
            "\n",
            "type:\n",
            "  Number of unique values: 2\n",
            "  Unique values: ['conventional', 'organic']\n",
            "\n",
            "year:\n",
            "  Number of unique values: 5\n",
            "  Unique values: ['1904', '2015', '2016', '2017', '2018']\n",
            "\n",
            "region:\n",
            "  Number of unique values: 54\n",
            "  Unique values: ['Albany', 'Atlanta', 'BaltimoreWashington', 'Boise', 'Boston', 'BuffaloRochester', 'California', 'Charlotte', 'Chicago', 'CincinnatiDayton', 'Columbus', 'DallasFtWorth', 'Denver', 'Detroit', 'GrandRapids', 'GreatLakes', 'HarrisburgScranton', 'HartfordSpringfield', 'Houston', 'Indianapolis', 'Jacksonville', 'LasVegas', 'LosAngeles', 'Louisville', 'MiamiFtLauderdale', 'Midsouth', 'Nashville', 'NewOrleansMobile', 'NewYork', 'Northeast', 'NorthernNewEngland', 'Orlando', 'Philadelphia', 'PhoenixTucson', 'Pittsburgh', 'Plains', 'Portland', 'RaleighGreensboro', 'RichmondNorfolk', 'Roanoke', 'Sacramento', 'SanDiego', 'SanFrancisco', 'Seattle', 'SouthCarolina', 'SouthCentral', 'Southeast', 'Spokane', 'StLouis', 'Syracuse', 'Tampa', 'TotalUS', 'West', 'WestTexNewMexico']\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "numeric_vars = df.select_dtypes(include=[np.number]).columns.tolist() # separate numeric variables\n",
        "categorical_vars = df.select_dtypes(include=['object']).columns.tolist() # separate categorical variables\n",
        "\n",
        "print(\"\\nNumeric Variables:\")\n",
        "\n",
        "for var in numeric_vars:\n",
        "    print(f\"\\n{var}:\") # list numerical variables\n",
        "    print(f\"  Range:       [{df[var].min():.2f}, {df[var].max():.2f}]\") # calculate range, mean, and median for each variable\n",
        "    print(f\"  Mean:        {df[var].mean():.2f}\")\n",
        "    print(f\"  Median:      {df[var].median():.2f}\")\n",
        "\n",
        "    skewness = df[var].skew() # calculates skewness of variable\n",
        "    if abs(skewness) < 0.5: # checks if skewness is close to 0\n",
        "        dist = \"approximately normal\"\n",
        "    elif skewness > 0.5:\n",
        "        dist = \"right-skewed (most values on left, long tail on right)\"\n",
        "    else:\n",
        "        dist = \"left-skewed (most values on right, long tail on left)\"\n",
        "\n",
        "    print(f\"  Distribution: {dist}\")\n",
        "\n",
        "print(\"\\nCategorical Variables:\")\n",
        "\n",
        "for var in categorical_vars: # list categorical variables\n",
        "    print(f\"\\n{var}:\")\n",
        "\n",
        "    unique_values = df[var].unique() # find unique categorical values\n",
        "    print(f\"  Number of unique values: {len(unique_values)}\")\n",
        "\n",
        "    unique_list = [str(x) for x in unique_values if pd.notna(x)] # list unique categorical values\n",
        "    print(f\"  Unique values: {sorted(unique_list)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAYz1-vDeYz-"
      },
      "source": [
        "3.2 Inspect the earliest recorded date\n",
        "- Find the earliest `Date`.\n",
        "- Check if there are avocado prices recorded from the earliest date up to 2010.\n",
        "- Comment: does the earliest data point look reasonable? Keep or remove?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "tjdWkRxweYFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfc0f62e-a67d-478b-d231-345ff53386bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Earliest date: 1904-01-21 00:00:00\n",
            "\n",
            "Records up to 2010: 1\n",
            "\n",
            "Earliest record(s):\n",
            "            Date  AveragePrice  Total Volume     type            region\n",
            "18250 1904-01-21          1.87      13766.76  organic  WestTexNewMexico\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df['Date'] = pd.to_datetime(df['Date']) # convert date to datetime to use min() (sorts chronologically)\n",
        "\n",
        "earliest_date = df['Date'].min() # find earliest date in \"Date\"\n",
        "\n",
        "print(f\"Earliest date: {earliest_date}\")\n",
        "\n",
        "before_2011 = df[df['Date'] <= pd.Timestamp('2010-12-31')] # check for dates before 2011\n",
        "print(f\"\\nRecords up to 2010: {len(before_2011)}\")\n",
        "\n",
        "earliest_records = df[df['Date'] == earliest_date] # show earliest records\n",
        "print(f\"\\nEarliest record(s):\")\n",
        "print(earliest_records[['Date', 'AveragePrice', 'Total Volume', 'type', 'region']])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This data point looks unreasonable. The date is so far from other entries in the dataset and has little relevance here. We should remove it as it would not have a major effect on data and would likely improve data quality."
      ],
      "metadata": {
        "id": "HR66UiFVRhcF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYYXKOJDf-bt"
      },
      "source": [
        "3.3 Highest average price\n",
        "- Find the highest value in \"AveragePrice\".\n",
        "- Report which region it belongs to.\n",
        "- Describe how you obtained the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "OMfJk4NKf6Vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa62fe2f-5ee8-4144-9c99-e892a8dae3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest Average Price: $3.25\n",
            "Region: SanFrancisco\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "max_price_idx = df['AveragePrice'].idxmax() # finds the index for maxium entry in \"AveragePrice\"\n",
        "max_price_row = df.loc[max_price_idx] # finds the row the maximum average price belongs to\n",
        "\n",
        "print(f\"Highest Average Price: ${max_price_row['AveragePrice']:.2f}\") # prints the maximum\n",
        "print(f\"Region: {max_price_row['region']}\") # prints the region"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used idxmax() to get the index of the entry of with the maximum average price. Then, I get the row that contains the maximum average price using the index and .loc(). Finally, I print the \"AveragePrice\" entry and the \"region\" entry from this row."
      ],
      "metadata": {
        "id": "SrkVb6CtSlc7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwRFtc8YgCBO"
      },
      "source": [
        "3.4 Highest total volume\n",
        "- Find the highest total volume of avocados.\n",
        "- Report which region it belongs to.\n",
        "- Describe how you obtained the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "tYzZo6C4gQym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f655b963-4a37-4722-9393-c010a9dd799c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highest Total Volume: 62505646.52\n",
            "Region: TotalUS\n"
          ]
        }
      ],
      "source": [
        "# Write your code here\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "max_volume_idx = df['Total Volume'].idxmax() # finds the index for maxium entry in \"Total Volume\"\n",
        "max_volume_row = df.loc[max_volume_idx] # finds the row the maximum total volume belongs to\n",
        "\n",
        "print(f\"Highest Total Volume: {max_volume_row['Total Volume']:.2f}\") # prints the maximum\n",
        "print(f\"Region: {max_volume_row['region']}\") # prints the region"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used idxmax() to get the index of the entry of with the maximum volume. Then, I get the row that contains the maximum volume using the index and .loc(). Finally, I print the \"Total Volume\" entry and the \"region\" entry from this row."
      ],
      "metadata": {
        "id": "Gl6xfFXZXd2s"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}